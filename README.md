# module11-challenge
Repository for module 11: Data Collection challenge.

## Table of Contents
- [About](#about)
- [Getting Started](#getting_started)
- [Installing](#installing)
- [Usage](#usage)
- [Contributing](#contributing)
## About
This assignment for the Data Collection challenge tasks us with using Splinter and BeautifulSoup to automate visiting a mars website, scraping articles and preview text from the website, storing the titles and previews inside a dictionary, and then closing the browser. In part two, we are tasked with automating visiting a different website with a table inside, extracting the table and converting into a dataframe, convert the data types of each column, analyze the data, and then export the table into a csv.
## Installiing
Notebook is under file "Christian_Smith_code" and is formatted to run within a Jupyter Notebook.
## Usage
To operate notebook code, open Jupyter Notebooks and navigate to the file "Christian_Smith_code". 
## Contributing
Main author: Christian Smith
Also received support from tutor David Pecot.
Line 28 in part two (relating to converting data types of the columns) was taken from https://www.geeksforgeeks.org/change-data-type-for-one-or-more-columns-in-pandas-dataframe/#